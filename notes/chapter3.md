#  Chapter3-线性模型

线性模型试图学得一个通过对属性线性组合来进行预测的函数，基本形式如下：

$$ f(x) = w^T x + b$$

确定了 $w$ 和 $b$，则模型就确定下来。

优点：形式简单，易于建模；可解释性强：**$w$ 表达了各个属性在预测中的重要性**。

## 线性回归

我们可以用线性模型来做简单的回归任务，即根据样本的属性预测样本的标签值。

均方误差是回归任务中常用的性能度量，其几何意义是**欧几里得距离**。

基于均方误差求解的方法即为最小二乘法，目标是找出一条直线使所有样本到直线距离之和最小。

广义线性模型：

$$ y  = g^{-1}(w^Tx+b)$$

其中 $g(\cdot)$ 连续且充分光滑，$g(\cdot) = ln(\cdot)$ 则为对数线性回归。 

## 对数几率回归

将回归任务和分类任务连接起来的，是对数几率函数：

$\sigma(x) = \frac{1}{1 +e ^{-x}}$

易得：

$$ ln \frac{y}{1-y} = w^T x+ b$$ 

解释：$y $ 为正例的概率， $1-y$ 为反例的可能性，二者的比值 $\frac{y}{1-y}$ 称为几率（odds），反应了 $x$ 作为正例的**相对可能性**，取对数即为对数几率函数。

实际上，我们是用线性回归的预测结果去逼近真实标记的对数几率。

优点：

1. 直接对分类可能性进行建模，无需假设数据分布，避免了假设不正确带来的问题
2. 不仅预测出类别，同时得到近似概率预测
3. 对数几率函数任意阶可导，便于求解

## LDA

线性判别分析（Linear Discriminant Analysis）是一种经典的线性学习方法，其思想是：给定训练集，将样本投影到一条直线上，使得**同类别样本的投影点尽可能接近、不同类别的投影点尽可能远离**；对新样本进行预测时，先进行投影，再根据投影点进行预测。

最大化目标（以二分类为例）：

1. 定义类内散度距离 $S_w$ 为：第 0 类样本距离  0 类中心的距离之和 + 第  1 类样本距离第 1 类中心的距离之和
2. 定义类间散度距离 $S_b$ 为：第 0 类中心和第 1 类中心的距离
3. 最大化：$ J = \frac{w^TS_bw}{w^TS_ww}$，即 $S_b$ 与 $S_w$ 的广义瑞利商（PS：不知道和大物的瑞利判据有没有什么关系）

求解 $w$ 的方法：**解与 $w$ 长度无关，只关乎其方向（投影到直线上，$w$ 决定其方向）**，使用拉格朗日乘子法解得：

$$ w = S_w^{-1}(\mu_0 - \mu_1)$$ $u_0$ 和 $\mu_1$ 为两个类别中心，实作中考虑到数值稳定性先对 $S_w$ 进行奇异值分解得到 $S_w = U\Sigma U^T$ 后再通过 $S_w^{-1} = U\Sigma^{-1} U^T$ 求得。

推广到多分类中：

1. 修改类内散度 $S_w$ 为各个类别距离该类中心距离之和（2 -> N）
2. 类间散度 $ S_b$ ：各个类到样本中心距离之和 $S_t$  - 类内散度$S_w$ 

通过找到 W 来 maximize  $\frac{tr(W^TS_bW)}{tr(W^TS_wW)}$ 完成多分类任务，其闭式解为 $S_w^{-1}S_b$ 的 $d'$ 个最大非零广义特征值对应的特征向量构成的矩阵（PS：没看懂）

**若将 $W$ 视作一个投影矩阵，那么 LDA 事实上将样本投影到 $d'$ 维空间，$d'$ 通常小于原有属性数目 $d$**。因此，LDA 同时也成为一种经典的监督将为技术，来将高维向量投影至低维空间并保留其部分类别信息（Word Embedding 的可视化就可以使用 LDA）。

## 多分类学习

一些二分类学习方法可以直接推广到多分类，比如 LDA，但在很多情况下，我们会将多分类问题**转化为二分类**进而来解决。如何转化，或者说将多分类任务拆分为多个二分类任务是关键。最经典的拆分策略有三种：OvO（一对一）、OvR（一对其余）以及MvM（多对多）。

1. OvO：将 N 个类别两两配对（这个表示戳中萌点2333），产生 N(N-1)/2 个分类器，使用这些分类器对样本进行预测，最终结果通过投票产生：取预测得最多的类别作为分类结果。（PS：这个表示好萌啊）
2. OvR：每次将一个类作为正例，剩余作为反例，训练 N 个分类器，测试时若只有一个分类器预测为正例，则类别为该分类器对应为正例的类别；若有多个，根据置信度来挑选。
3. MvM：若干个类别作为正例，若干个作为反例，其构造需要有特殊的设计，常用的技术有纠错输出码。
   1. 编码： 对 N 个类别做 M 次划分，每次划分一部分作为正类一部分反类，得到 M 个训练集和 M 个分类器
   2. 解码：用 M 个分类器进行预测，预测表及组成一个编码，用该编码和类别编码进行比较，返回距离最小的作为结果

## 类别不平衡的处理方法

训练集中的样本类别数目如果有较大差距，对于学习会产生很大影响，例如，10000 样本中 只有 2 个反例，那么即使预测全为正例的分类器也能达到 99.8% 的准确率，但是这没有任何意义。这种情况就称之为类别不平衡（class-imbalance）。

根据之前的内容，我们可以得到，分类器分类的是用预测的 $y$ 值同一个阈值进行比较，若大于阈值则判定为正例，反之为反例。比如，阈值设置为 0.5，即认为正例和反例出现可能性相同，即$ \frac{y}{1-y} > 1$ 时判定为正例。

但是类别不平衡时，假设正例数目为 $m^+$，反例为 $m^-$，则观测几率为 $\frac{m^+}{m^-}$。若我们认为训练集是对真实样本的**无偏采样**，则观测几率代表真实几率。于是，当正例出现的相对几率大于真实几率时，判定为正例，即：

$$  \frac{y}{1-y} > \frac{m^+}{m^-}$$

我们通过使用再平衡（rescaling）， 来把右侧的这个 `1` 调整为观测几率：

$ \frac{y'}{1-y'} = \frac{y}{1-y} \times \frac{m^+}{m^-}$

这种手段称之为**阈值移动**

还有两种通过人为改变训练集的类别比例来消除类别不平衡的影响的手段：

1. 欠采样（undersampling）：去除一些出现较多的类别的样本，使得两个类别数目相近
2. 过采样（oversampling）：增加一些出现较少的类别的样本

注意点：

1. 欠采样速度比过采样快，因为一个是减小训练集一个是增加
2. 过采样不能简单的重复采样，否则会产生严重的过拟合
3. 欠采样的丢弃可能会造成信息的丢失
4. 再缩放技术和 Cost-Sensitive 有关联，是后者的基础

