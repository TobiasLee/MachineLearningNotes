# Chapter 5 神经网络

## 神经元模型

与生物中神经元兴奋类似，“M-P神经元模型”接到来自其他 n 个神经传过来的信号， 经过带权重的连接进行传递，总输入值将于阈值进行比较，而后通过激活函数。

$$ y = f( \sum_ {i = 1}^{n} w_i x_i - \theta)$$

常用的激活函数：

1. 阶跃函数：$ f(x)  = 1 \ if \  x >= 0 \ else\  0 $
2. Sigmoid： $ f(x) = \frac{1}{1 + e^{-x}} $
3. ReLU $f(x) = max(0, x)$
4.  tanh $f(x) = tanh(x)$

## 感知机与多层网络

感知机由两层神经元组成（无隐藏层），输入层接收信号传递给输出层（是一个 M-P 神经元）

感知机能能学习**与 、 或、非**（线性可分），但是不能模拟**异或**（非线性可分）

学习方式：

$$ w_i = w_i + \Delta w_i, \Delta w_i = \alpha (y - \hat y) x_i$$

其中 $\alpha$ 是学习率 

通过增加隐藏层，我们可以解决非线性可分问题。神经网络的学习，实质上是学习到了**权值和阈值**。

## 误差反向传播 Back Propagation

推导实例略，核心思路：

1. 根据误差函数，借助链式法则，逐层求偏导数
2. Sigmoid 的导数 $f'(x) = f(x) ( 1- f(x))$

累积 BP 和标准 BP：

标准 BP 每次更新只针对一个样例，在训练初期可能会更新频繁并且存在大量抵消；累积 BP 读取整个训练集之后才更新，开始下降迅速，但后期训练缓慢。

BP 神经网络易遭受过拟合，解决策略：

1. Early Stopping
2. Regularization，可以在误差函数中增加权重的正则项

## Local Optima and Global Optima

参数空间内梯度为零的点，就是局部极小点；而一个函数可能有个局部最小，全局最小一定是局部最小，但反之不然。如何跳出局部最小：

- 采取不同初始化，选取最优局部最小
- 模拟退火
- 使用 SGD，引入随机因素

## Deep Learning

深度学习：很深层的神经网络，训练开销大，可以考虑：

1. 无监督逐层训练，最后 BP 整个网络
2. 权值共享：CNN、RNN

深度学习的理解：对输入信号逐层加工，将其转化成与输出目标更加密切的表示。**低层特征向高层特征表示的转化**，再使用简单的 model 来完成复杂的任务。



