# Chapter-2 模型的评估与选择

## 过拟合

学习器在训练集上的误差称为训练误差，在新样本上的误差称为泛化误差。

过拟合（overfitting）：把训练样本自身的特点当做所有潜在样本具有的一般性质，泛化性能下降。

欠拟合（underfitting）：对训练样本的一般性质尚未学好。

**过拟合无法避免**：机器学习面临的问题通常是 NP 难甚至更难，而有效的算法必然在多项式时间内完成，若能彻底避免过拟合，则通过最小化经验误差就能够获得最优解，即构造性证明了 `P=NP`；只要相信 `P!=NP`，则过拟合不可避免。

## 评估方法

使用测试集来测试学习期对新样本的判别能力，其中的测试样本尽量不在训练集中出现。产生训练集 S 和 T 常用的方法有留出法、交叉验证（k 折交叉验证）和自助法。

### 留出法

直接将数据集 D 划分成两个互斥的集合 S 和 T，在 S 上训练，在 T 上测试。

注意：

1. S 和 T 的划分尽可能保持数据分布的一致性，避免因划分引入偏差。在分类任务中，样本类别比例相似，采用**分层采样**。
2. 样本划分时的分布同样有影响。采用若干次随机划分，重复试验评估取平均值。

窘境：我们希望用 D 来训练，但 S 和 T 的划分使得训练集变小，若 T 较大，则训练结果与用 D 训练差别较大 ；若 T 较小，训练的模型接近 D，但是评估效果不准。

### 交叉验证法

1. 将 D 划分成 k 个大小相似的互斥子集，每个子集 $D_i$ 都尽可能保持数据分布的一致性，通过分层采样得到。
2. 每次用 k -1 个子集的并集作为训练集，余下的作为测试集，进行 k 次训练
3. 结果为 k 次的均值

k 常用的取值为 10。为减少划分引入的差别，通常随机划分 p 次，做 p 次 k 折交叉验证，结果取均值。

#### 留一法

假设 D 中包含 m 个样本，若令 `k = m`，则得到交叉验证法的一个特例，留一法（Leave-One-Out, LOO）。

优点：实际评估的模型与 D 非常相似，结果比较准确（不一定，NFL 同样适用评估方法）

缺点：计算开销在数据集较大时难以忍受。

### 自助法

留出法和交叉验证都保留了一部分样本用于测试，导致训练集实际上比 D 小，这会引入因**训练样本规模不同而导致的估计偏差**。自助法（boostrapping）是一种解决方案。

我们通过对有 m 个样本的 D 进行 m 次采样得到 `D'`，用 `D'` 作为训练集，`D\D'` 作为测试集。显然，有些样本在 `D'` 中出现多次，有些一次也不出现。

样本在 m 次采样中都不被采样到的概率为 $(1-\frac{1}{m})^m$ ，取极限得到其概率为 $\frac{1}{e}=0.368$，即 D 中约有 37% 的样本未出现在 `D'` 中。依旧有不曾出现在训练集中的样本用于测试，这样的测试结果成为“包外估计”。

优点：在数据集较小难以划分 T 和 S 时很有用；同时能够产生多个不同的训练集，对集成学习有帮助

缺点：自助法产生的数据集**改变了初始数据集的分布**（样本出现的次数发生改变），会引入估计偏差

因此在样本数量足够时，多采用留出法和交叉验证法。

## 性能度量

回归任务常用的均方误差：

$$ E(f; D) = \frac{1}{m} \sum_{i=1}^m (f(\mathbf{x}_i)-y_i)^2$$

分类任务常用的性能度量有：Error Rate、Accuracy、Precision、Recall、F1、ROC、AUC

### Error Rate and Accuracy

错误率（Error Rate）定义为：

$$ E(f; D) = \frac{1}{m} \sum_{i=1}^m \mathbf{I}(f(\mathbf{x}_i)!=y_i)$$

$\mathbf{I}(*)$ 为指示函数，若 `*` 成立则为 1 否则为 0.

精度定义为：

$$acc(f;D) = = \frac{1}{m} \sum_{i=1}^m \mathbf{I}(f(\mathbf{x}_i)=y_i) =1 - E(f;D)$$ 

### Precision, Recall and F1

| 真实情况 | 预测结果 | 预测结果 |
| :------: | :------: | :------: |
|          |   正例   |   反例   |
|   正例   |    TP    |    FN    |
|   反例   |    FP    |    TN    |

Precision 查准率：$ P = \frac{TP}{TP + FP}$ ，查询结果中有多少是正确的

Recall 查全率（召回率）：$R = \frac{TP}{TP + FN}$ ，查询结果占正例的比例

二者是一对**矛盾**的度量，鱼和熊掌不可兼得（除非任务非常简单）。

PR曲线：我们根据学习器的预测结果对样例进行排序，排在最前面的是认为“最可能”的正例样本，最后的是“最不可能”是正例的样本。按此顺序逐个把样本作为正例进行预测，可以计算出当前的 P 和 R，从而绘制出 P-R 曲线。

若一个 PR 曲线能够完全包住另一个，则前者的性能优于后者。PR 曲线下面积也是一个度量指标，但因其不便计算，引入一些其他的度量方式：

1. BEP（Break-Even Point），P = R 时的取值，越高越好
2. F1 Score，P 和 R 的调和平均值，即 $ F = \frac{2PR}{P + R} = \frac{2 TP}{样本总数+ TP - TN}$

可以通过对 P 和 R 进行加权来调整 F1，得到一个加权 $F_{\beta}$：

$$ F_{\beta} = \frac{1 + \beta^2PR}{(\beta^2P) + R}$$

$\beta > 1$ 时，更看重查全率（逃犯检索）；$\beta < 1$，侧重查准率，例如推荐系统。

### ROC and AUC

很多学习器根据分类阈值来进行分类，例如，若预测值大于分类阈值则为正例，反之为反例。在不同的任务中，可以设置不同的阈值来调整结果以适应需求。

我们可以根据学习器预测结果对样例排序，排序的质量好坏，体现了考虑学习器在不同任务下的“期望泛化性能“的好坏。ROC 即是从这个角度来度量学习器泛化性能的有力工具。

类似 RP 曲线的绘制，对排序结果逐个进行整理的预测，计算下列两个指标分别作为纵轴和横轴：

$ TPR = \frac{TP}{TP + FN}$ ，真正例率，计算公式和 Recall 相同

$ FRP = \frac{FP}{TN + FP} $ ，假正例率，易得，$FRP + Precision = 1$

 同样，若一个 ROC 包住另一个，则前者性能优于后者；若发生交叉，则使用 ROC 曲线下面积 AUC 来比较优劣。

## 偏差与方差

我们可以通过”偏差-方差分解“来解释算法泛化性能。

泛化误差 $E(f;D)$ 可以分解为：

$$ E(f;D) = bias^2(\mathbf{x}) + var(\mathbf{x}) + \epsilon ^ 2$$

即泛化误差由偏差、方差以及噪声之和。

偏差项度量了算法的期望预测和真实结果的偏离程度，即刻画**算法的拟合能力**

方差项度量了同样大小训练集的变动导致的性能变化，即刻画**数据扰动所造成的影响**

噪声表达了任何算法能够达到的泛化误差的下界，噪声可能是错误的 label 等带来的。

偏差和方差同样存在冲突：训练不足的时候，算法拟合能力弱，数据扰动不足以对产生显著影响；随着训练加深，算法拟合能力逐渐增强，数据的扰动能显著地影响算法的性能，则方差项主导泛化错误率。

## 标准化

Min-max 和 Z-score 标准化的优劣：

Min-max：min-max标准化方法是对原始数据进行线性变换。设 minA 和 maxA 分别为属性 A的最小值和最大值，将A的一个原始值x通过min-max标准化映射成在区间 [0,1] 中的值x'，其公式为：新数据=（原数据-最小值）/（最大值-最小值）

优点：**线性变换，计算方便**

缺点：对**离群点（特别大或者特别小的点）敏感**，新加入数据需要重新计算 max/min。

Z-score（零均值）：这种方法基于原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。将A的原始值x使用z-score标准化到x'。新数据=（原数据-均值）/标准差。

优点：**适用 max/min 未知，对离群点不敏感**

缺点：处理后数据符合正态分布，**可能改变数据的分布情况**；均值、方差计算可能较为繁琐。